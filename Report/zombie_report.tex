\documentclass[]{article}

\setlength{\parskip}{1em}
\usepackage{amssymb}
\usepackage[ruled]{algorithm2e}
\usepackage{natbib, color}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{
	left=32.5mm,
	right=32.5mm,
	top=32.5mm
}
\usepackage{natbib}
\bibliographystyle{apalike}

%opening
\title{Modelling a Zombie Apocalypse with ABC}
\author{Dan Milner, Harry Tata, Hannah Sansford}

\begin{document}

\maketitle

\section{Introduction}

(General background info)Chaotic dynamic systems commensurate with ecology and epidemiology challenge conventional methods of statistical inference as oftentimes they have an intractable likelihood. It is unlikely that any natural/environmental process under investigation can be captured without error and all natural/environmental systems invariably suffer process stochasticity. Consequently, model complexity increases in a non-trivial way as each realisation of the model is essentially unique. For chaotic models this is especially true as process stochasticity induces divergence of paths generated using identical parameters and starting from the sme initial conditions (Fasiolo *et al*, 2016).

(Specific background Info) Zombies, particularly when instigating an apocalypse, exhibit just such chaotic behavior. Their aim is to kill, eat and infect people via a bit/bites, which leave an open wound with the zombie's saliva in and around it. This bodily fluid mixes with the blood, infecting the (previously susceptible) bitten individual and turns them into a zombie. Zombie apocalypses have been modeled on a number of previous occasions (Munz *et al*, ) but given the seriousness of such a situation it is important that no stone is left un-turned. 

(A description of the gap in our knowledge that this study is designed to fill) Statistical methods capable of dealing effectively with highly non-linear systems are not a trivial matter. The simulation-based methods, Sequential Monte Carlo (SMC), Approximate Bayesian Computation (ABC) and Synthetic Likelihood (SL), offer a solution through estimating a posterior via simulated data sets based on sample parameters taken from the prior distribution. Utilising the computational efficiency of SMC, this report aims to compute a 4-Class zombie apocalypse model by approximating the posterior using a progressively decreasing sequence of tolerances. 


\section{Model}
We consider four basic classes:
\begin{itemize}
	\item Susceptible ($S$)
	\item Infected ($I$)
	\item Zombie ($Z$)
	\item Removed ($R$)
\end{itemize}

Susceptibles class can become deceased through 'natural' causes (parameter $\delta$). Removed class are those that have died either through a zombie attack or from natural causes. If a human has died in a zombie attack they can resurrect and become a zombie (parameter $\zeta$). A Susceptible can become a zombie through transmission, i.e. an encounter with a zombie (parameter $\beta$). Zombies can, therefore, only come from two sources; either they are resurrected from the newly deceased or they are a susceptible that has become infected. Prior to becoming a zombie, the period of time for which a susceptibles is infected lasts approximately 24 hours. Infected individuals can still die of natural causes before becoming a zombie, otherwise they become a zombie.

The birth rate of the human population is assumed to constant, $\Pi$. Zombies can be defeated by removing the head or destroying the brain (parameter $\alpha$). 

The 4-Class model is thus:

\[S' = \Pi - \beta SZ - \delta S\]
\[I' = \beta SZ - \rho I - \delta I\]
\[Z' = \beta SZ + \zeta R - \alpha SZ\]
\[R' = \delta S + \alpha SZ - \zeta R\]

\section{Approximate Bayesian Computation}

Likelihood-free inference methods are convenient for complex models, such as Epidemic models, where we do not have an explicit expression for the likelihood. In approximate Bayesian computation (ABC), simulation under the implicit model replaces computation of the likelihood. One can use simulations from the model for different parameter values to compare the simulated datasets with the observed data. These simulations are increasingly being used as training datasets for machine learning methods including deep neural networks \citep{RN6} and random forests \citep{RN8}. 

The use of ABC first became popular in the field of population genetics, where simulation from a range of models is possible, but the likelihood is intractable for realistic sized datasets. \cite{RN57} were the first to use ABC, conducting inference on human demographic history. The method has now been applied in various subject areas including systems biology \citep{RN60}, climate modelling \citep{RN61}, astronomy \citep{RN62} and epidemiology \citep{RN10}.

The goal of ABC is to find a posterior distribution for the parameters of the implicit model, explaining the complex and potentially high-dimensional dataset. The method is based on Bayesian statistics: updating our prior beliefs about the model parameters, where $\pi(\theta)$ is the prior, using the information from our simulations. Suppose the dataset consists of $n$ observations $y_{obs} = (y_{obs,1}, ..., y_{obs,n})$. A typical ABC procedure would involve mapping the observations to a lower dimensional set of summary statistics, $s(y)$. The posterior is then proportional to the following elements
\begin{equation}
p_{\epsilon}(\theta, s|s_{obs}) \propto \pi(\theta)f_n(s|\theta)K_{\epsilon}(\|s - s_{obs}\|),
\end{equation}
where $f_n(s|\theta)$ is the implicit density of the model, $K_{\epsilon}(x)$ is a kernel function with tolerance $\epsilon$, and $\| \cdot \|$ is a distance metric \citep{RN2}. The kernel function enables us to include in our posterior density the parameter values that best approximate the observations. This idea of estimating the likelihood of parameters using simulations that are `close' to the observed data dates back at least as far as \cite{RN56}.

As the tolerance value $\epsilon$ tends to zero, the simulations we accept are closer to the true data and the approximate posterior becomes closer to its true distribution. Unfortunately, in order to simulate the same number of points at a smaller tolerance level often requires many more simulations, and the simplest algorithms can quickly become computationally inefficient. Luckily, we can harness techniques such as Markov chain Monte Carlo \citep{RN17, RN27} and sequential Monte Carlo \citep{RN21, RN30, RN22, RN29} to improve the computational efficiency of ABC.

\subsection{Rejection algorithm}
\label{sec1}

Instead of pre-defining the tolerance level $\epsilon$, it is sometimes beneficial to simulate a set number of datasets $n$, of which we retain the proportion $p$ closest to the observed data. This version of the rejection algorithm is preferential in the scenario where one is unsure of a suitable tolerance to use a priori. The algorithm used in this report is outlined below:

\begin{algorithm}[H]
	\label{ABC Rejection} 
	\caption{ABC Rejection}
	\For{$i\leftarrow 1$ \KwTo $n$}{
		Sample $\theta_i \sim \pi(\theta)$\;
		Simulate data $y_i \sim f(y|\theta_i)$\;
		Transform data $y_i$ into summary statistics $s_i$ (optional)\;
		Calculate distance $\rho(s_i, s_{obs})$
	}
	Retain proportion $p$ with smallest distance
\end{algorithm}

\noindent Hence, in simple terms, the rejection algorithm consists of sampling parameters from a prior distribution and simulating datasets from a model using these parameters. One can then, optionally, transform these datasets into summary statistics, before retaining the proportion $p$ closest to the observations. The accepted sample of parameters then approximates the posterior distribution.

\subsection{Application to Zombie model}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{../Figures/rej_ABC_simulations}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{../Figures/rej_abc_sd_bands}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{../Figures/rejection_posteriors}
\end{figure}


\section{Sequential Monte Carlo}

Sequential Monte Carlo (SMC) methods applied to ABC are concerned with reducing the number of model runs in order to achieve a certain quality of posterior approximation. We have now seen that the simple ABC rejection algorithm is very computationally demanding, which often limits applications to simple models; the number of simulations required to sample the entire parameter space grows exponentially with the number of parameters \citep{RN32}. SMC methods aim to progressively approximate the posterior using a decreasing sequence of tolerance levels $\{\epsilon_1, ..., \epsilon_T\}$. The idea is to sample from areas of the parameter space with a higher likelihood, rather than systematically from the whole space, in order to gain computational efficiency.

The basis of SMC algorithms is to construct a sequence of target distributions, that one propagates a set of particles through in order to move from the initial distribution to the final distribution. 

\subsection{Population Monte Carlo}

The first stage of the PMC ABC method of \cite{RN21} is identical to that of rejection ABC in Algorithm \ref{ABC Rejection}, giving the first stage approximation to the posterior distribution. One then resamples parameter values from a density kernel 
$$ q(\theta) = \sum_{j=1}^{N} w_j^{(t-1)} K(\theta| \theta_{j}^{(t-1)};\tau_t^2),$$ 
with $N$ being the number of samples in the first, and subsequent, iterations. A popular choice is a Gaussian kernel and \cite{RN21} shows that a good choice of $\tau$ is twice the empirical variance of the simulated parameters in the previous iteration. The bias that is induced by sampling from a proposal distribution, rather than the prior, is fixed by assigning an importance weight to each particle, as outlined in Algorithm \ref{PMC ABC} below. 


\begin{algorithm}[H]
	\label{PMC ABC}
	\caption{Population Monte Carlo ABC (PMC)}
	Given a decreasing sequence of tolerance levels $\{\epsilon_1, ..., \epsilon_T\}$\;
	\For{$t=1$}{
		\For{$i=1$ to $N$}{
			\textbf{until} $\rho(y, y_{obs}) < \epsilon_1$\;
			Simulate $\theta_{i}^{(1)} \sim \pi(\theta)$ and $y \sim f(y|\theta_{i}^{(1)})$ }
		Set $w_{i}^{(1)} = 1/N$ }
	\For{$t=2$ to $T$}{
		\For{$i=1$ to $N$}{
			\textbf{until} $\rho(y, y_{obs}) < \epsilon_t$\;
			Sample $\theta_{i}^{*}$ from $\theta^{(t-1)}_j$ with probabilities $w_j^{(t-1)}$\;
			Generate $\theta_{i}^{(t)} \sim K(\theta| \theta_{i}^{*};\tau_t^2)$ and $y \sim f(y|\theta_{i}^{(t)})$ }
		Set $w_{i}^{(t)} \propto \pi(\theta_{i}^{(t)}) / \sum_{j=1}^{N} w_j^{(t-1)} K(\theta_{i}^{(t)}| \theta_j^{(t-1)}; \tau_t^2)$\;
		Set $\tau_{t+1}^2$ as twice the weighted emprirical variance of the $\theta_i^{(t)}$'s}
\end{algorithm}	

A drawback of this method is that it requires the user to pre-define a decreasing sequence of tolerance levels $\{\epsilon_1, ..., \epsilon_T\}$. A poor choice of sequence could detriment the possible benefits of the importance sampling procedure. In the population genetic example conducted by \cite{RN21}, $\epsilon_1$ is based on a preliminary simulation as the 0.1 quantile and they perform four iterations with $\epsilon_2 = 0.75\epsilon_1$, $\epsilon_3 = 0.9\epsilon_2$ and $\epsilon_4 = 0.9\epsilon_3$.

\subsection{Application to Zombie model}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{../Figures/PMC-ABC_simulations}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{../Figures/PMC_ABC_sd_bands}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{../Figures/PMC_posteriors}
\end{figure}


\bibliography{references}

\end{document}
