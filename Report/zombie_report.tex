\documentclass[]{article}

\setlength{\parskip}{1em}
\usepackage{amssymb}
\usepackage[ruled]{algorithm2e}
\usepackage{natbib, color}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{
	left=32.5mm,
	right=32.5mm,
	top=32.5mm
}
\usepackage{natbib}
\bibliographystyle{apalike}

%opening
\title{Modelling a Zombie Apocalypse with ABC}
\author{Dan Milner, Harry Tata, Hannah Sansford}

\begin{document}

\maketitle

\section{Introduction}

(General background info)Chaotic dynamic systems commensurate with ecology and epidemiology challenge conventional methods of statistical inference as oftentimes they have an intractable likelihood. It is unlikely that any natural/environmental process under investigation can be captured without error and all natural/environmental systems invariably suffer process stochasticity. Consequently, model complexity increases in a non-trivial way as each realisation of the model is essentially unique. For chaotic models this is especially true as process stochasticity induces divergence of paths generated using identical parameters and starting from the sme initial conditions (Fasiolo *et al*, 2016).

(Specific background Info) Zombies, particularly when instigating an apocalypse, exhibit just such chaotic behavior. Their aim is to kill, eat and infect people via a bit/bites, which leave an open wound with the zombie's saliva in and around it. This bodily fluid mixes with the blood, infecting the (previously susceptible) bitten individual and turns them into a zombie. Zombie apocalypses have been modeled on a number of previous occasions (Munz *et al*, ) but given the seriousness of such a situation it is important that no stone is left un-turned. 

(A description of the gap in our knowledge that this study is designed to fill) Statistical methods capable of dealing effectively with highly non-linear systems are not a trivial matter. The simulation-based methods, Sequential Monte Carlo (SMC), Approximate Bayesian Computation (ABC) and Synthetic Likelihood (SL), offer a solution through estimating a posterior via simulated data sets based on sample parameters taken from the prior distribution. Utilising the computational efficiency of SMC, this report aims to compute a 4-Class zombie apocalypse model by approximating the posterior using a progressively decreasing sequence of tolerances. 


\section{Model}
We consider four basic classes:
\begin{itemize}
	\item Susceptible ($S$)
	\item Infected ($I$)
	\item Zombie ($Z$)
	\item Removed ($R$)
\end{itemize}

Susceptibles class can become deceased through 'natural' causes (parameter $\delta$). Removed class are those that have died either through a zombie attack or from natural causes. If a human has died in a zombie attack they can resurrect and become a zombie (parameter $\zeta$). A Susceptible can become a zombie through transmission, i.e. an encounter with a zombie (parameter $\beta$). Zombies can, therefore, only come from two sources; either they are resurrected from the newly deceased or they are a susceptible that has become infected. Prior to becoming a zombie, the period of time for which a susceptibles is infected lasts approximately 24 hours. Infected individuals can still die of natural causes before becoming a zombie, otherwise they become a zombie.

The birth rate of the human population is assumed to constant, $\Pi$. Zombies can be defeated by removing the head or destroying the brain (parameter $\alpha$). 

The 4-Class model is thus:

\[S' = \Pi - \beta SZ - \delta S\]
\[I' = \beta SZ - \rho I - \delta I\]
\[Z' = \beta SZ + \zeta R - \alpha SZ\]
\[R' = \delta S + \alpha SZ - \zeta R\]

\section{Approximate Bayesian Computation}

Likelihood-free inference methods are convenient for complex models, such as Epidemic models, where we do not have an explicit expression for the likelihood. In approximate Bayesian computation (ABC), simulation under the implicit model replaces computation of the likelihood. One can use simulations from the model for different parameter values to compare the simulated datasets with the observed data. These simulations are increasingly being used as training datasets for machine learning methods including deep neural networks \citep{RN6} and random forests \citep{RN8}. 

The use of ABC first became popular in the field of population genetics, where simulation from a range of models is possible, but the likelihood is intractable for realistic sized datasets. \cite{RN57} were the first to use ABC, conducting inference on human demographic history. The method has now been applied in various subject areas including systems biology \citep{RN60}, climate modelling \citep{RN61}, astronomy \citep{RN62} and epidemiology \citep{RN59,RN10}.

The goal of ABC is to find a posterior distribution for the parameters of the implicit model, explaining the complex and potentially high-dimensional dataset. The method is based on Bayesian statistics: updating our prior beliefs about the model parameters, where $\pi(\theta)$ is the prior, using the information from our simulations. Suppose the dataset consists of $n$ observations $y_{obs} = (y_{obs,1}, ..., y_{obs,n})$. A typical ABC procedure would involve mapping the observations to a lower dimensional set of summary statistics, $s(y)$. The posterior is then proportional to the following elements
\begin{equation}
p_{\epsilon}(\theta, s|s_{obs}) \propto \pi(\theta)f_n(s|\theta)K_{\epsilon}(\|s - s_{obs}\|),
\end{equation}
where $f_n(s|\theta)$ is the implicit density of the model, $K_{\epsilon}(x)$ is a kernel function with tolerance $\epsilon$, and $\| \cdot \|$ is a distance metric \citep{RN2}. The kernel function enables us to include in our posterior density the parameter values that best approximate the observations. This idea of estimating the likelihood of parameters using simulations that are `close' to the observed data dates back at least as far as \cite{RN56}.

As the tolerance value $\epsilon$ tends to zero, the simulations we accept are closer to the true data and the approximate posterior becomes closer to its true distribution. Unfortunately, in order to simulate the same number of points at a smaller tolerance level often requires many more simulations, and the simplest algorithms can quickly become computationally inefficient. Many new techniques have been proposed over the past couple of decades in order to improve the computational efficiency and quality of inference of ABC. From Markov chain Monte Carlo \citep{RN17, RN27} and sequential Monte Carlo methods \citep{RN21, RN30, RN22, RN29}, to reduction of the dimension of summary statistics \citep{RN4, RN3, RN49, RN11} through a wide variety of techniques.

\bibliography{references}

\end{document}
